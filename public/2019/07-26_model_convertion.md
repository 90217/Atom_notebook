# ___2019 - 07 - 26 Model Convertion___
***

- [numpy.linalg.norm](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.norm.html)
- [Python Numpy计算各类距离](https://blog.csdn.net/liukuan73/article/details/80494779)
- [相似度计算方法](https://www.cnblogs.com/chenxiangzhen/p/10648503.html)

- [MTCNN人脸及特征点检测---代码应用详解（基于ncnn架构）](https://blog.csdn.net/fuwenyan/article/details/77573755)
- [MTCNN解读：Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](https://blog.csdn.net/fuwenyan/article/details/73201680)
- [Modify From skimage to opencv warpaffine](https://github.com/cftang0827/face_alignment/commit/ae0fac4aa1e5658aa74027ec28eab876606c505e)
- [Image processing for text recognition](http://blog.mathocr.com/2017/06/25/image-processing-for-text-recognition.html)

- [使用 GPU](https://www.tensorflow.org/guide/using_gpu)
- [tensorflow中的batch_normalization实现](https://www.cnblogs.com/jiangxinyang/p/9394353.html)
- [谈谈Tensorflow的Batch Normalization](https://www.jianshu.com/p/0312e04e4e83)
- [docker/tensorflow/tensorflow](https://hub.docker.com/r/tensorflow/tensorflow/)

- [weakref](https://docs.python.org/3/library/weakref.html)
# MMDNN
## 安装
  - [microsoft/MMdnn](https://github.com/microsoft/MMdnn)
  - **安装**
    ```sh
    pip install mmdnn
    pip install -U git+https://github.com/Microsoft/MMdnn.git@master
    ```
## 基本命令
  - **模型可视化** [MMDNN Visualizer](http://mmdnn.eastasia.cloudapp.azure.com:8080/)
    ```sh
    mmdownload -f keras -n inception_v3
    mmtoir -f keras -w imagenet_inception_v3.h5 -o keras_inception_v3
    ```
    选择文件 keras_inception_v3.json
  - **mmdownload** 下载预训练好的模型
    ```py
    # 返回框架支持的模型
    mmdownload -f tensorflow

    # 下载指定的模型
    mmdownload -f tensorflow -n inception_v3
    ```
  - **mmvismeta** 可以使用 tensorboard 作为后端将计算图可视化
    ```py
    mmvismeta imagenet_inception_v3.ckpt.meta ./log
    ```
  - **mmtoir** 将模型转化为中间表达形式 IR (intermediate representation)，结果中的 json 文件用于可视化，proto / pb 用于描述网络结构模型，npy 用于保存网络数值参数
    ```py
    mmtoir -f tensorflow -n imagenet_inception_v3.ckpt.meta  -w inception_v3.ckpt --dstNode MMdnn_Output -o converted
    ```
  - **mmtocode** 将 IR 文件转化为指定框架下构造网络的原始代码，以及构建网络过程中用于设置权重的参数，结果生成一个 py 文件，与 npy 文件一起用于模型迁移学习或模型推断
    ```py
    mmtocode -f pytorch -n converted.pb -w converted.npy -d converted_pytorch.py -dw converted_pytorch.npy
    ```
  - **mmtomodel** 生成模型，可以直接使用对应的框架加载
    ```py
    mmtomodel -f pytorch -in converted_pytorch.py -iw converted_pytorch.npy -o converted_pytorch.pth
    ```
  - **mmconvert** 用于一次性转化模型，是 mmtoir / mmtocode / mmtomodel 三者的集成
    ```py
    mmconvert -sf tensorflow -in imagenet_inception_v3.ckpt.meta -iw inception_v3.ckpt --dstNode MMdnn_Output -df pytorch -om tf_to_pytorch_inception_v3.pth
    ```
  - **caffe alexnet -> tf tested**
    ```sh
    master branch with following scripts:
    $ python -m mmdnn.conversion._script.convertToIR -f caffe -d kit_imagenet -n examples/caffe/models/bvlc_alexnet.prototxt -w examples/caffe/models/bvlc_alexnet.caffemodel
    $ python -m mmdnn.conversion._script.IRToCode -f tensorflow --IRModelPath kit_imagenet.pb --dstModelPath kit_imagenet.py -w kit_imagenet.npy
    $ python -m mmdnn.conversion.examples.tensorflow.imagenet_test -n kit_imagenet.py -w kit_imagenet.npy --dump ./caffe_alexnet.ckpt
    Tensorflow file is saved as [./caffe_alexnet.ckpt], generated by [kit_imagenet.py] and [kit_imagenet.npy].
    ```
## mxnet 模型转化为 tensorflow
  ```sh
  cd model-r100-ii/
  mmtoir -f mxnet -n model-symbol.json -w model-0000.params -d resnet100 --inputShape 3,112,112
  mmtocode -f tensorflow --IRModelPath resnet100.pb --IRWeightPath resnet100.npy --dstModelPath tf_resnet100.py
  mmtomodel -f tensorflow -in tf_resnet100.py -iw resnet100.npy -o tf_resnet100 --dump_tag SERVING

  mmconvert -sf mxnet -in model-symbol.json -iw model-0000.params -df tensorflow -om resnet100 --dump_tag SERVING --inputShape 3,112,112
  ```
  ```py
  from tensorflow.python.platform import gfile
  from tensorflow.python.util import compat
  from tensorflow.core.protobuf import saved_model_pb2

  with gfile.FastGFile('./saved_model.pb', 'rb') as ff:
      data = compat.as_bytes(ff.read())
      sm = saved_model_pb2.SavedModel()
      sm.ParseFromString(data)

  g_in = tf.import_graph_def(sm.meta_graphs[0].graph_def)

  LOGDIR='./logdir'
  train_writer = tf.summary.FileWriter(LOGDIR)
  train_writer.add_graph(sess.graph)
  train_writer.flush()
  train_writer.close()

  tensorboard --logdir ./logdir/
  ```
## 加载模型用于迁移学习
  - IR 文件转化为 tensorflow 模型
    ```sh
    # IR 文件转化为模型代码
    mmtocode -f tensorflow --IRModelPath converted.pb --IRWeightPath converted.npy --dstModelPath tf_inception_v3.py

    # 修改 is_train 为 True
    sed -i 's/is_train = False/is_train = True/' tf_inception_v3.py

    # IR 文件转化为 tensorflow 模型
    mmtomodel -f tensorflow -in tf_inception_v3.py -iw converted.npy -o tf_inception_v3 --dump_tag TRAINING
    ```
  - 模型加载
    ```py
    export_dir = "./tf_inception_v3"
    with tf.Session(graph=tf.Graph()) as sess:
        tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.TRAINING], export_dir)

        x = sess.graph.get_tensor_by_name('input:0')
        y = sess.graph.get_tensor_by_name('xxxxxx:0')
        ......
        _y = sess.run(y, feed_dict={x: _x})
    ```
## MMdnn IR 层的表示方式
  - [IR 层的 proto 说明文件](https://github.com/Microsoft/MMdnn/blob/master/mmdnn/conversion/common/IR/graph.proto)
  - IR 文件包含以下几个部分
    - **GraphDef** NodeDef / version
    - **NodeDef** name / op / input / attr
    - **Attrvalue** list / type / shape / tensor
    - **Tensorshape**  dim
    - **LiteralTensor** type / tensor_shape / values
  - **Graph** 描述网络模型，内部包含若干 node，对应网络模型的层，node 中的 input 描述每一层之间的输入输出连接关系
  - **NodeDef**
    - **name** 本层 name，在 Graph 中唯一
    - **op** 本层算子，算子描述见链接，在算子描述文件中，算子 name 唯一
    - **input** 用于描述本层输入关系，各层之间的输入输出关系靠此成员描述
    - **attr** attr 成员可以是 listvalue / type / shape / tensor
      - **list** 存储 list 成员，如 list(int) / list(float) / list(shape) 等，**data** 保存数值数据，类型可以是 bytes / int64 / float / bool
      - **type** 描述数值类型
      - **shape** 描述 tensor 形状
      - **tensor** 各 node 之间传递的 tensor
  - **TensorShape** 描述 tensor 的维度信息
  - **LiteralTensor** 存储 tensor, 在各 node 之间传递
***

# ONNX
  - [onnx/onnx](https://github.com/onnx/onnx)
  - [onnx/models](https://github.com/onnx/models)
  - [onnx/tutorials](https://github.com/onnx/tutorials)
  - [MMdnn/mmdnn/conversion/onnx/](https://github.com/microsoft/MMdnn/tree/master/mmdnn/conversion/onnx)
  - [Importing an ONNX model into MXNet](http://mxnet.incubator.apache.org/versions/master/tutorials/onnx/super_resolution.html)
  - ONNX为AI模型提供了一个开源格式。 它定义了一个可扩展的计算图模型，以及内置运算符和标准数据类型的定义，MMdnn也将支持ONNX格式
  ```sh
  pip install onnx
  ```
## mxnet-model-server and ArcFace-ResNet100 (from ONNX model zoo)
  - [ArcFace-ResNet100 (from ONNX model zoo)](https://github.com/awslabs/mxnet-model-server/blob/master/docs/model_zoo.md/#arcface-resnet100_onnx)
  - [onnx/models/vision/body_analysis/arcface/](https://github.com/onnx/models/tree/master/vision/body_analysis/arcface)
  - [awslabs/mxnet-model-server](ttps://github.com/awslabs/mxnet-model-server)
  ```sh
  pip install mxnet-model-server
  mxnet-model-server --start --models arcface=https://s3.amazonaws.com/model-server/model_archive_1.0/onnx-arcface-resnet100.mar

  curl -O https://s3.amazonaws.com/model-server/inputs/arcface-input1.jpg

  curl -O https://s3.amazonaws.com/model-server/inputs/arcface-input2.jpg

  curl -X POST http://127.0.0.1:8080/predictions/arcface -F "img1=@arcface-input1.jpg" -F "img2=@arcface-input2.jpg"

  mxnet-model-server --stop
  ```
## onnx模型转换为Tensorflow模型
  - [Can not use converted ONNX -> TF graph independently ](https://github.com/onnx/onnx-tensorflow/issues/167)
  - [Train in Tensorflow, Export to ONNX](https://github.com/onnx/tutorials/blob/master/tutorials/OnnxTensorflowExport.ipynb)
    ```py
    import onnx
    import numpy as np
    from onnx_tf.backend import prepare

    model = onnx.load('./assets/mnist_model.onnx')
    tf_rep = prepare(model)

    img = np.load("./assets/image.npz")
    output = tf_rep.run(img.reshape([1, 1,28,28]))

    print("outpu mat: \n",output)
    print("The digit is classified as ", np.argmax(output))

    import tensorflow as tf
    with tf.Session() as persisted_sess:
        print("load graph")
        persisted_sess.graph.as_default()
        tf.import_graph_def(tf_rep.predict_net.graph.as_graph_def(), name='')
        inp = persisted_sess.graph.get_tensor_by_name(
            tf_rep.predict_net.tensor_dict[tf_rep.predict_net.external_input[0]].name
        )
        out = persisted_sess.graph.get_tensor_by_name(
            tf_rep.predict_net.tensor_dict[tf_rep.predict_net.external_output[0]].name
        )
        res = persisted_sess.run(out, {inp: img.reshape([1, 1,28,28])})
        print(res)
        print("The digit is classified as ",np.argmax(res))

    tf_rep.export_graph('tf.pb')
    ```
  - 转换完成后，需要对转换出的tf.pb模型进行验证，验证方式如下：
    ```py
    import numpy as np
    import tensorflow as tf
    from tensorflow.python.platform import gfile

    name = "tf.pb"

    with tf.Session() as persisted_sess:
        print("load graph")
        with gfile.FastGFile(name, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())

        persisted_sess.graph.as_default()
        tf.import_graph_def(graph_def, name='')

        inp = persisted_sess.graph.get_tensor_by_name('0:0')
        out = persisted_sess.graph.get_tensor_by_name('LogSoftmax:0')
        #test = np.random.rand(1, 1, 28, 28).astype(np.float32)
        #feed_dict = {inp: test}

        img = np.load("./assets/image.npz")
        feed_dict = {inp: img.reshape([1, 1,28,28])}

        classification = persisted_sess.run(out, feed_dict)
        print(out)
        print(classification)
    ```
## mxnet 加载 ONNX 模型
  ```py
  import onnx
  from onnx_tf.backend import prepare

  model = onnx.load('./resnet100.onnx')
  tf_rep = prepare(model)
  help(prepare)

  import mxnet as mx
  import mxnet.contrib.onnx as onnx_mxnet

  sym, arg, aux = onnx_mxnet.import_model('./resnet100.onnx')
  all_layers = sym.get_internals()
  sym = all_layers["fc1_output"]
  ctx = mx.gpu(0)
  model = mx.mod.Module(symbol=sym, context=ctx, label_names=None)
  model.bind(data_shapes=[("data", (1, 3, 112, 112))])
  model.set_params(arg, aux)

  from mtcnn.mtcnn import MTCNN
  import cv2
  from src import face_preprocess
  import numpy as np
  img = cv2.imread('./3.jpg')
  img_str = cv2.imencode('.png',img)[1]
  frame = cv2.imdecode(img_str, cv2.IMREAD_COLOR)

  detector = MTCNN(steps_threshold=[0.6, 0.7, 0.7])
  ret = detector.detect_faces(frame)
  bbox = np.array(
      [
          [ii["box"][0], ii["box"][1], ii["box"][2] + ii["box"][0], ii["box"][3] + ii["box"][1], ii["confidence"]]
          for ii in ret
      ]
  )
  points = np.array([list(ii["keypoints"].values()) for ii in ret])

  nimg = face_preprocess.preprocess(frame, bbox[0], points[0], image_size="112,112")
  aligned = np.transpose(nimg, (2, 0, 1))
  input_blob = np.expand_dims(aligned, axis=0)
  data = mx.nd.array(input_blob)
  db = mx.io.DataBatch(data=(data,))
  model.forward(db, is_train=False)

  data_names = [graph_input for graph_input in sym.list_inputs()
                        if graph_input not in arg and graph_input not in aux]
  mod = mx.mod.Module(symbol=sym, data_names=data_names, context=mx.gpu(0), label_names=None)


  # forward on the provided data batch
  mod.forward(Batch([mx.nd.array(test_image)]))
  mod.get_outputs()
  ```
***

# Tensorflow SavedModel 模型的保存与加载
## SavedModel 保存与加载
  - **SavedModel** 是一种独立于语言且可恢复的序列化格式，使较高级别的系统和工具可以创建、使用和转换 TensorFlow 模型，包含完整的 TensorFlow 程序，包括权重和计算图，可以使用 python 训练模型，然后在 Java 中非常方便的加载模型
  - TensorFlow 提供了多种与 SavedModel 交互的方式，包括 `tf.saved_model API` / `tf.estimator.Estimator` 和命令行界面，如使用 `TensorFlow Serving` 将训练好的模型部署至生产环境
  - 一个比较完整的 SavedModel 模型文件夹包含以下内容
    ```sh
    ├── assets  # 可选，可以添加可能需要的外部文件
    ├── assets.extra  # 可选，是一个库，可以添加其特定assets的地方
    ├── saved_model.pb  # 是 MetaGraphDef，包含图形结构，MetaGraphDef 是MetaGraph的Protocol Buffer表示
    └── variables # 保存训练所习得的权重
        ├── variables.data-00000-of-00001
        └── variables.index
    ```
  - **tf.saved_model.simple_save** 简单保存，创建 SavedModel 的最简单方法
    ```py
    tf.saved_model.simple_save(sess, "./model", inputs={"myInput": x, "Input_2": z}, outputs={"myOutput": y})
    ```
  - **加载** 加载后作为特定 MetaGraphDef 的一部分提供的变量、资源和签名子集将恢复到提供的会话中
    ```py
    # Loads the model from a SavedModel as specified by tags. (deprecated)
    # This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
    load(sess, tags, export_dir, import_scope=None, **saver_kwargs)
    ```
    ```py
    export_dir = ...
    ...
    with tf.Session(graph=tf.Graph()) as sess:
      tf.saved_model.loader.load(sess, [tag_constants.TRAINING], export_dir)
      ...
    ```
  - **MNIST 示例**
    ```py
    mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

    with tf.Session(graph=tf.Graph()) as sess:
      tf.saved_model.loader.load(sess, ["serve"], "./model")
      graph = tf.get_default_graph()

      input = np.expand_dims(mnist.test.images[0], 0)
      x = sess.graph.get_tensor_by_name('myInput:0')
      y = sess.graph.get_tensor_by_name('myOutput:0')
      batch_xs, batch_ys = mnist.test.next_batch(1)
      scores = sess.run(y,
               feed_dict={x: batch_xs})
      print("predict: %d, actual: %d" % (np.argmax(scores, 1), np.argmax(batch_ys, 1)))    
    ```
  - 在 **TensorFlow Serving** 中加载和提供 SavedModel
    ```sh
    tensorflow_model_server --port=port-numbers --model_name=your-model-name --model_base_path=your_model_base_path
    ```
  - **simple_save** 配置 SavedModel 的模型能够通过 `TensorFlow Serving` 进行加载，并支持 `Predict API`，要访问 `classify API` / `regress API` / `multi-inference API`，需要使用 builder API 或 tf.estimator.Estimator 手动构建 SavedModel
## 使用 SavedModelBuilder 保存模型
  - **tf.saved_model.builder.SavedModelBuilder** 构造 SavedModelBuilder 对象，提供了保存多个 MetaGraphDef 的功能，初始化方法只需要传入用于保存模型的目录名，目录不用预先创建
    ```py
    class tf.saved_model.builder.SavedModelBuilder
    __init__(export_dir)
    ```
    - **MetaGraph** 是一种数据流图，并包含相关变量、资源和签名
    - **MetaGraphDef** 是 MetaGraph 的协议缓冲区表示法
    - **签名** 是一组与图有关的输入和输出
  - **add_meta_graph_and_variables** 方法导入graph的信息以及变量，这个方法假设变量都已经初始化好了，对于每个 SavedModelBuilder 一定要执行一次用于导入第一个meta graph
    ```py
    # 导入graph与变量信息
    add_meta_graph_and_variables(
            self, sess, tags, signature_def_map=None, assets_collection=None,
            legacy_init_op=None, clear_devices=False, main_op=None,
            strip_default_attrs=False, saver=None)
    ```
    - **sess** 当前的session，包含 graph 的结构与所有变量
    - **tags** 给当前需要保存的 meta graph 一个标签，在载入模型的时候，需要根据这个标签名去查找对应的 MetaGraphDef，找不到会报 RuntimeError
    - **signature_def_map** 定义模型的 Signature，指定模型的输入 / 输出 tensor 等
    - 通过 `strip_default_attrs=True` 确保向前兼容性
  - 必须使用用户指定的 **标签** 对每个添加到 SavedModel 的 MetaGraphDef 进行标注
    - 标签提供了一种方法来识别要加载和恢复的特定 MetaGraphDef，以及共享的变量和资源子集
    - 标签一般会标注 MetaGraphDef 的功能（例如服务或训练），有时也会标注特定的硬件方面的信息（如 GPU）
    - 标签可以选用系统定义好的参数，如 `tf.saved_model.tag_constants.SERVING` 与 `tf.saved_model.tag_constants.TRAINING`
  - **save** 将模型序列化到指定目录底下，保存好以后，目录下会有一个 saved_model.pb 文件以及 variables 文件夹
    ```py
    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)
    builder.add_meta_graph_and_variables(sess, ['tag_string'])
    builder.save()
    ```
  - 使用 SavedModelBuilder 构建 SavedModel 的典型方法
    ```py
    export_dir = ...
    ...
    builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
    with tf.Session(graph=tf.Graph()) as sess:
      ...
      builder.add_meta_graph_and_variables(sess,
                                           [tag_constants.TRAINING],
                                           signature_def_map=foo_signatures,
                                           assets_collection=foo_assets,
                                           strip_default_attrs=True)
    ...
    # Add a second MetaGraphDef for inference.
    with tf.Session(graph=tf.Graph()) as sess:
      ...
      builder.add_meta_graph([tag_constants.SERVING], strip_default_attrs=True)
    ...
    builder.save()
    ```
  - **tf.saved_model.loader.load** 载入模型
    ```py
    tf.saved_model.loader.load(sess, tags, export_dir, import_scope=None, **saver_kwargs)
    ```
    ```py
    meta_graph_def = tf.saved_model.loader.load(sess, ['tag_string'], saved_model_dir)
    ```
  - load 完以后，可以从 sess 对应的 graph 中获取需要的 tensor 来 inference
    ```py
    x = sess.graph.get_tensor_by_name('input_x:0')
    y = sess.graph.get_tensor_by_name('predict_y:0')

    # 实际的待inference的样本
    _x = ...
    sess.run(y, feed_dict={x: _x})
    ```
## 指定 Signature 保存模型
  - add_meta_graph_and_variables 的参数可以指定博阿村模型时的 Signature
    ```py
    tf.saved_model.signature_def_utils.build_signature_def(inputs=None, outputs=None, method_name=None)

    # 构建tensor info
    tf.saved_model.utils.build_tensor_info(tensor)
    ```
    - inputs / outputs 都是 dict，key 是约定的输入输出别名，value 是对具体 tensor 包装得到的 TensorInfo
  - 典型用法
    ```py
    builder = tf.saved_model.builder.SavedModelBuilder(saved_model_dir)
    # x 为输入tensor, keep_prob为dropout的prob tensor
    inputs = {'input_x': tf.saved_model.utils.build_tensor_info(x),
                'keep_prob': tf.saved_model.utils.build_tensor_info(keep_prob)}

    # y 为最终需要的输出结果tensor
    outputs = {'output' : tf.saved_model.utils.build_tensor_info(y)}

    signature = tf.saved_model.signature_def_utils.build_signature_def(inputs, outputs, 'test_sig_name')

    builder.add_meta_graph_and_variables(sess, ['test_saved_model'], {'test_signature':signature})
    builder.save()
    ```
    - add_meta_graph_and_variables 的 signature_def_map 参数接收的是一个 dict，key 是自己命名的 signature 名称，value 是 SignatureDef 对象
  - **模型载入**
    ```py
    ## 略去构建sess的代码

    signature_key = 'test_signature'
    input_key = 'input_x'
    output_key = 'output'

    meta_graph_def = tf.saved_model.loader.load(sess, ['test_saved_model'], saved_model_dir)
    # 从meta_graph_def中取出SignatureDef对象
    signature = meta_graph_def.signature_def

    # 从signature中找出具体输入输出的tensor name
    x_tensor_name = signature[signature_key].inputs[input_key].name
    y_tensor_name = signature[signature_key].outputs[output_key].name

    # 获取tensor 并inference
    x = sess.graph.get_tensor_by_name(x_tensor_name)
    y = sess.graph.get_tensor_by_name(y_tensor_name)

    # _x 实际输入待inference的data
    sess.run(y, feed_dict={x:_x})
    ```
## SavedModel 中的 SignatureDefs 定义类型
  - **SignatureDefs** 定义函数的输入输出，在使用 SavedModel 时可以指定
  - SignatureDef 结构
    - **inputs** TensorInfo 的字典格式
    - **outputs** TensorInfo 的字典格式
    - **method_name** 用于加载时的方法名称
    - TensorInfo 包含 tensor 名称 name / 类型 device_type / 维度 / shape 等信息
  - **Classification SignatureDef** 支持 TensorFlow Serving 的分类 API 调用，其中 Input tensor 是必需的，两个 output Tensors 至少有一个
    ```py
    signature_def: {
      key  : "my_classification_signature"
      value: {
        inputs: {
          key  : "inputs"
          value: {
            name: "tf_example:0"
            dtype: DT_STRING
            tensor_shape: ...
          }
        }
        outputs: {
          key  : "classes"
          value: {
            name: "index_to_string:0"
            dtype: DT_STRING
            tensor_shape: ...
          }
        }
        outputs: {
          key  : "scores"
          value: {
            name: "TopKV2:0"
            dtype: DT_FLOAT
            tensor_shape: ...
          }
        }
        method_name: "tensorflow/serving/classify"
      }
    }
    ```
  - **Predict SignatureDef** 支持 TensorFlow Serving 的预测 API 调用，可以使用任意数量的 input / output tensor，其中 output 还可以添加额外的 tensor
    ```py
    signature_def: {
      key  : "my_prediction_signature"
      value: {
        inputs: {
          key  : "images"
          value: {
            name: "x:0"
            dtype: ...
            tensor_shape: ...
          }
        }
        outputs: {
          key  : "scores"
          value: {
            name: "y:0"
            dtype: ...
            tensor_shape: ...
          }
        }
        method_name: "tensorflow/serving/predict"
      }
    }
    ```
  - **Regression SignatureDef** 支持 TensorFlow Serving 的回归 API 调用，必须只有一个 input / output tensor
    ```py
    signature_def: {
      key  : "my_regression_signature"
      value: {
        inputs: {
          key  : "inputs"
          value: {
            name: "x_input_examples_tensor_0"
            dtype: ...
            tensor_shape: ...
          }
        }
        outputs: {
          key  : "outputs"
          value: {
            name: "y_outputs_0"
            dtype: DT_FLOAT
            tensor_shape: ...
          }
        }
        method_name: "tensorflow/serving/regress"
      }
    }
    ```
## C++ 加载 SavedModel 模型
  - **C++ 加载模型** SavedModel 加载后的版本称为 SavedModelBundle，其中包含 MetaGraphDef 和加载时所在的会话。
    ```c
    const string export_dir = ...
    SavedModelBundle bundle;
    ...
    LoadSavedModel(session_options, run_options, export_dir, {kSavedModelTagTrain}, &bundle);
    ```
## 使用 keras 训练与保存模型 pb 文件
  - **在 Fashion MNIST 上训练 keras 分类器**
    ```py
    from tensorflow import keras

    # Error in 1.14: could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
    # set_session is removed in tf 2.0
    config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))
    sess = tf.Session(config=config)
    keras.backend.set_session(sess)

    fasion_mnist = keras.datasets.fashion_mnist
    (train_images, train_labels), (test_images, test_labels) = fasion_mnist.load_data()
    print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)
    # (60000, 28, 28) (60000,) (10000, 28, 28) (10000,)

    train_images = train_images / 255.0
    test_images = test_images / 255.0
    train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)
    test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)
    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
    print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)
    # (60000, 28, 28, 1) (60000,) (10000, 28, 28, 1) (10000,)

    model = keras.Sequential([
        keras.layers.Conv2D(input_shape=(28, 28, 1), filters=8, kernel_size=3, strides=2, activation='relu', name='Conv1'),
        keras.layers.Flatten(),
        keras.layers.Dense(10, activation=tf.nn.softmax, name="Softmax")
    ])
    model.summary()

    testing = False
    epochs = 5
    model.compile(optimizer=tf.train.AdamOptimizer(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    model.fit(train_images, train_labels, epochs=epochs)

    test_loss, test_acc = model.evaluate(test_images, test_labels)
    print('\nTest accuracy: {}'.format(test_acc))
    # Test accuracy: 0.8719000220298767
    ```
  - **保存模型 pb 文件**
    ```py
    # Fetch the Keras session and save the model
    # The signature definition is defined by the input and output tensors,
    # and stored with the default serving key
    import tempfile

    MODEL_DIR = tempfile.mktemp()
    version = 1
    export_path = os.path.join(MODEL_DIR, str(version))
    print('export_path = {}\n'.format(export_path))
    # export_path = /tmp/tmp36fw8qod/1

    if os.path.isdir(export_path):
      print('\nAlready saved a model, cleaning up\n')
      !rm -r {export_path}

    tf.saved_model.simple_save(
        keras.backend.get_session(),
        export_path,
        inputs={'input_image': model.input},
        outputs={t.name:t for t in model.outputs})

    print('\nSaved model:')
    !ls -l {export_path}
    # Saved model:
    # total 72
    # -rw-r--r-- 1 leondgarse leondgarse 65984 八月 13 12:00 saved_model.pb
    # drwxr-xr-x 2 leondgarse leondgarse  4096 八月 13 12:00 variables
    ```
  - **模型测试与 serving**
    ```py
    !saved_model_cli show --dir {export_path} --all

    %%bash --bg
    nohup tensorflow_model_server \
      --rest_api_port=8501 \
      --model_name=fashion_model \
      --model_base_path="${MODEL_DIR}" >server.log 2>&1
    ```
***

# TF 2.0 Beta 使用 SavedModel 格式
## TensorFlow 2.0 安装
    ```sh
    conda create -n tf-20 python=3.7
    conda activate tf-20

    pip install tensorflow==2.0.0-beta1
    pip install tensorflow-gpu==2.0.0-beta1

    conda install ipython
    conda install pandas matplotlib pillow

    ipython
    ```
    ```py
    tf.__version__
    # Out[1]: '2.0.0-beta1'
    ```
## 保存与加载 keras 模型
  - **获取 keras mobilenet 模型**
    ```py
    file = tf.keras.utils.get_file(
        "grace_hopper.jpg",
        "https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg")
    img = tf.keras.preprocessing.image.load_img(file, target_size=[224, 224])
    plt.imshow(img)
    plt.axis('off')

    x = tf.keras.preprocessing.image.img_to_array(img)
    x = tf.keras.applications.mobilenet.preprocess_input(
        x[tf.newaxis,...])
    ```
    ![](images/tf_serve_grace_hopper.jpg)
  - **模型使用与保存 tf.saved_model.save**
    ```py
    #tf.keras.applications.vgg19.decode_predictions
    labels_path = tf.keras.utils.get_file('ImageNetLabels.txt','https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt')
    imagenet_labels = np.array(open(labels_path).read().splitlines())

    pretrained_model = tf.keras.applications.MobileNet()
    result_before_save = pretrained_model(x)
    decoded = imagenet_labels[np.argsort(result_before_save)[0,::-1][:5]+1]
    print("Result before saving:", decoded)
    # Result before saving:
    #  ['military uniform' 'bow tie' 'suit' 'bearskin' 'pickelhaube']

    tf.saved_model.save(pretrained_model, "/tmp/mobilenet/1/")
    ```
    其中保存模型的路径需要有一个版本号 `/1`，用于 Tensorflow Serving 加载
  - **saved_model_cli show 命令** 查看 pb 模型，saved_model 的 `signatures` 方法保存了模型信息，keras 将前向传输过程保存在 `serving_default` 签名键下
    ```py
    !saved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve --signature_def serving_default
    # The given SavedModel SignatureDef contains the following input(s):
    #   inputs['input_1'] tensor_info:
    #       dtype: DT_FLOAT
    #       shape: (-1, 224, 224, 3)
    #       name: serving_default_input_1:0
    # The given SavedModel SignatureDef contains the following output(s):
    #   outputs['act_softmax'] tensor_info:
    #       dtype: DT_FLOAT
    #       shape: (-1, 1000)
    #       name: StatefulPartitionedCall:0
    # Method name is: tensorflow/serving/predict
    ```
  - **模型加载 tf.saved_model.load**
    ```py
    loaded = tf.saved_model.load("/tmp/mobilenet/1/")
    print(list(loaded.signatures.keys()))
    # ["serving_default"]

    infer = loaded.signatures["serving_default"]
    print(infer.structured_outputs)
    # {'act_softmax': TensorSpec(shape=(None, 1000), dtype=tf.float32, name='act_softmax')}

    labeling = infer(tf.constant(x))[pretrained_model.output_names[0]]
    decoded = imagenet_labels[np.argsort(labeling)[0,::-1][:5]+1]
    print("Result after saving and loading:\n", decoded)
    # Result after saving and loading:
    #  ['military uniform' 'bow tie' 'suit' 'bearskin' 'pickelhaube']
    ```
## SavedModel 文件结构
  - SavedModel 目录包含序列化后的模型 / 参数与单词表，serialized signatures / variable / vocabularies
    ```sh
    ls /tmp/mobilenet/1
    # assets  saved_model.pb  variables
    ```
  - **saved_model.pb** 包含 named signatures，定义了模型的不同功能
    ```sh
    saved_model_cli show --dir /tmp/mobilenet/1 --tag_set serve
    # The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:
    # SignatureDef key: "__saved_model_init_op"
    # SignatureDef key: "serving_default"
    ```
  - **variables** 目录包含一个标准的训练过程中的 checkpoint
    ```sh
    ls /tmp/mobilenet/1/variables
    # variables.data-00000-of-00001  variables.index
    ```
  - **assets** 目录包含 TensorFlow graph 会使用的文件，如初始化单词表的文件
  - **assets.extra** 目录包含任何 TensorFlow graph 不用的文件，如接口调用示例等
## tensorflow_model_server 使用模型启动服务
  - **启动服务**
    ```sh
    echo "deb [arch=amd64] http://storage.googleapis.com/tensorflow-serving-apt stable tensorflow-model-server tensorflow-model-server-universal" | sudo tee /etc/apt/sources.list.d/tensorflow-serving.list
    curl https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg | sudo apt-key add -
    sudo apt update

    sudo apt-get install tensorflow-model-server

    nohup tensorflow_model_server \
        --rest_api_port=8501 \
        --model_name=mobilenet \
        --model_base_path="/tmp/mobilenet" >server.log 2>&1
    ```
  - **发送请求**
    ```py
    import json
    import numpy
    import requests

    data = json.dumps({"signature_name": "serving_default", "instances": x.tolist()})
    headers = {"content-type": "application/json"}
    json_response = requests.post('http://localhost:8501/v1/models/mobilenet:predict', data=data, headers=headers)
    predictions = numpy.array(json.loads(json_response.text)["predictions"])
    ```
## 导出自定义模型
  - **tf.function** 对于自定义的模型，需要指定 signature 中用于提供服务的函数接口，其他没有修饰的函数，加载后将不可见
    ```py
    class CustomModule(tf.Module):

      def __init__(self):
        super(CustomModule, self).__init__()
        self.v = tf.Variable(1.)

      @tf.function
      def __call__(self, x):
        return x * self.v

      @tf.function(input_signature=[tf.TensorSpec([], tf.float32)])
      def mutate(self, new_v):
        self.v.assign(new_v)

    module = CustomModule()
    ```
  - **input_signature** 指定函数调用时的输入形式，对于没有指定的函数，需要在保存之前调用过，加载时将使用模型保存之前的输入形式
    ```py
    module(tf.constant(0.))
    tf.saved_model.save(module, "/tmp/module_no_signatures")
    ```
  - **tf.saved_model.load** 加载模型
    ```py
    imported = tf.saved_model.load("/tmp/module_no_signatures")
    print(imported(tf.constant(3.)).numpy())
    # 3.0

    imported.mutate(tf.constant(2.))
    print(imported(tf.constant(3.)).numpy())
    # 6.0

    # ValueError: Could not find matching function to call loaded from the SavedModel.
    imported(tf.constant([3.]))
    ```
  - **get_concrete_function** 可以在不调用模型方法的情况下，指定方法的输入形式，包括类型 / 维度 / 自定义名称，维度信息可以是 None，表示任意维度
    ```py
    module.__call__.get_concrete_function(x=tf.TensorSpec([None], tf.float32))
    tf.saved_model.save(module, "/tmp/module_no_signatures")
    imported = tf.saved_model.load("/tmp/module_no_signatures")
    print(imported(tf.constant([3.])).numpy())
    # [3.]
    ```
  - **saved_model_cli** 查看模型信息
    ```sh
    saved_model_cli show --dir /tmp/module_no_signatures --tag_set serve
    # The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:
    # SignatureDef key: "__saved_model_init_op"

    saved_model_cli show --dir /tmp/module_no_signatures --all
    ```
## 指定输出的接口功能 signature
  - 通过在保存时通过指定 `signatures` 参数，指定作为接口功能的函数
    ```py
    call = module.__call__.get_concrete_function(tf.TensorSpec(None, tf.float32))
    tf.saved_model.save(module, "/tmp/module_with_signature", signatures=call)    

    !saved_model_cli show --dir /tmp/module_with_signature --tag_set serve --signature_def serving_default
    # The given SavedModel SignatureDef contains the following input(s):
    #   inputs['x'] tensor_info:
    #       dtype: DT_FLOAT
    #       shape: unknown_rank
    #       name: serving_default_x:0
    # The given SavedModel SignatureDef contains the following output(s):
    #   outputs['output_0'] tensor_info:
    #       dtype: DT_FLOAT
    #       shape: unknown_rank
    #       name: StatefulPartitionedCall:0
    # Method name is: tensorflow/serving/predict
    ```
  - **模型导入**
    ```py
    imported = tf.saved_model.load("/tmp/module_with_signature")
    signature = imported.signatures["serving_default"]

    print(signature(x=tf.constant([3.]))["output_0"].numpy())
    # [3.]
    imported.mutate(tf.constant(2.))
    print(signature(x=tf.constant([3.]))["output_0"].numpy())
    # [6.]
    print(imported.v.numpy())
    # 2.
    ```
  - 默认的 signature 是 `serving_default`，可以通过指定一个字典输出多个 signature
    ```py
    @tf.function(input_signature=[tf.TensorSpec([], tf.string)])
    def parse_string(string_input):
      return imported(tf.strings.to_number(string_input))

    signatures = {"serving_default": parse_string, "from_float": imported.signatures["serving_default"]}
    tf.saved_model.save(imported, "/tmp/module_with_multiple_signatures", signatures)

    !saved_model_cli show --dir /tmp/module_with_multiple_signatures --tag_set serve
    # The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:
    # SignatureDef key: "__saved_model_init_op"
    # SignatureDef key: "from_float"
    # SignatureDef key: "serving_default"
    ```
  - **saved_model_cli** 也可以在命令行直接运行模型
    ```sh
    saved_model_cli run --dir /tmp/module_with_multiple_signatures --tag_set serve --signature_def serving_default --input_exprs="string_input='3.'"
    saved_model_cli run --dir /tmp/module_with_multiple_signatures --tag_set serve --signature_def from_float --input_exprs="x=3."
    ```
## 模型微调 Fine-tuning imported models
  - 导入模型的变参 Variable 可以通过反向传播调整
    ```py
    optimizer = tf.optimizers.SGD(0.05)

    def train_step():
        with tf.GradientTape() as tape:
            loss = (10. - imported(tf.constant(2.))) ** 2
        variables = tape.watched_variables()
        grads = tape.gradient(loss, variables)
        optimizer.apply_gradients(zip(grads, variables))
        return loss

    for _ in range(10):
        # "v" approaches 5, "loss" approaches 0
        print("loss={:.2f} v={:.2f}".format(train_step(), imported.v.numpy()))

    # loss=36.00 v=3.20
    # loss=12.96 v=3.92
    # ...
    # loss=0.01 v=4.97
    # loss=0.00 v=4.98
    ```
## Control flow in SavedModels
  - 任何可以进入 tf.function 的方法都可以保存到 SavedModel 中，如 python 的控制流
    ```py
    @tf.function(input_signature=[tf.TensorSpec([], tf.int32)])
    def control_flow(x):
      if x < 0:
        tf.print("Invalid!")
      else:
        tf.print(x % 3)

    to_export = tf.Module()
    to_export.control_flow = control_flow
    tf.saved_model.save(to_export, "/tmp/control_flow")

    imported = tf.saved_model.load("/tmp/control_flow")
    imported.control_flow(tf.constant(-1))  # Invalid!
    imported.control_flow(tf.constant(2))   # 2
    imported.control_flow(tf.constant(3))   # 0    
    ```
## Estimators 保存与加载 SavedModels
  - **tf.Estimator.export_saved_model** 导出 SavedModels
    ```py
    input_column = tf.feature_column.numeric_column("x")
    estimator = tf.estimator.LinearClassifier(feature_columns=[input_column])

    def input_fn():
        return tf.data.Dataset.from_tensor_slices(
            ({"x": [1., 2., 3., 4.]}, [1, 1, 0, 0])).repeat(200).shuffle(64).batch(16)
    estimator.train(input_fn)

    serving_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(
                              tf.feature_column.make_parse_example_spec([input_column]))
    export_path = estimator.export_saved_model("/tmp/from_estimator/", serving_input_fn)    
    ```
  - **tf.saved_model.load** 加载模型
    ```py
    imported = tf.saved_model.load(export_path)

    def predict(x):
        example = tf.train.Example()
        example.features.feature["x"].float_list.value.extend([x])
        return imported.signatures["predict"](examples=tf.constant([example.SerializeToString()]))

    print(predict(1.5))
    print(predict(3.5))    
    ```
  - **tf.estimator.export.build_raw_serving_input_receiver_fn** 可以用于创建输入功能
***

## png to jpg
  ```py
  from skimage.io import imread, imsave
  import glob2

  for pp in glob2.glob('*.png'):
      print(pp)
      jj = os.path.basename(pp).split('.')[0] + '.jpg'
      # jj = os.path.join('jpg', jj)
      img = imread(pp)
      if img.shape[2] == 4:
          img[img[:, :, 3] == 0] = [255, 255, 255, 255]
      imsave(jj, img[:, :, :3])
  ```
  ```sh
  find ./* -iname '*.png'
  grep -srinI '\.png)'
  sed -i 's/\.png)$/.jpg)/' ./*.md
  ```
## inspect_checkpoint
  ```py
  检查某个检查点中的变量
  我们可以使用 inspect_checkpoint 库快速检查某个检查点中的变量。

  继续前面所示的保存/恢复示例：

  # import the inspect_checkpoint library
  from tensorflow.python.tools import inspect_checkpoint as chkp

  # print all tensors in checkpoint file
  chkp.print_tensors_in_checkpoint_file("/tmp/model.ckpt", tensor_name='', all_tensors=True)

  # tensor_name:  v1
  # [ 1.  1.  1.]
  # tensor_name:  v2
  # [-1. -1. -1. -1. -1.]

  # print only tensor v1 in checkpoint file
  chkp.print_tensors_in_checkpoint_file("/tmp/model.ckpt", tensor_name='v1', all_tensors=False)

  # tensor_name:  v1
  # [ 1.  1.  1.]

  # print only tensor v2 in checkpoint file
  chkp.print_tensors_in_checkpoint_file("/tmp/model.ckpt", tensor_name='v2', all_tensors=False)

  # tensor_name:  v2
  # [-1. -1. -1. -1. -1.]
  ```
## insightface
  ```py
  sess = tf.InteractiveSession()
  meta_graph_def = tf.saved_model.loader.load(sess, ["serve"], "./")
  print(meta_graph_def.signature_def)

  x = sess.graph.get_tensor_by_name("data:0")
  y = sess.graph.get_tensor_by_name("fc1/add_1:0")

  from mtcnn.mtcnn import MTCNN
  from skimage.transform import resize

  img = plt.imread('./3.jpg')
  detector = MTCNN(steps_threshold=[0.6, 0.7, 0.7])
  ret = detector.detect_faces(img)

  bb = ret[0]['box']
  frame = img[bb[0]:bb[0] + bb[2], bb[1]:bb[1] + bb[3]]
  frame = resize(frame, (112, 112))

  frame = frame[np.newaxis, :, :, :]
  rr = sess.run(y, feed_dict={x: frame})
  ```
## keras session
  ```py
  import keras.backend.tensorflow_backend as KTF
  import tensorflow as tf
  config = tf.ConfigProto()
  config.gpu_options.allow_growth=True   
  sess = tf.Session(config=config)

  KTF.set_session(sess)
  ```
